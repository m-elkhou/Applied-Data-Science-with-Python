{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "Implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DNV-0yLb_lSA",
        "3jWjg2kJ_em8"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRkde7Uh-t6F",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<table align=\"center\" >\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th>\n",
        "                <a  href=\"http://www.fsdmfes.ac.ma/\">\n",
        "                    <img src=\"https://github.com/m-elkhou/Object-Detection/blob/master/assets/usmba.png?raw=1\" width=70px />\n",
        "                </a>\n",
        "            </th>\n",
        "            <th>\n",
        "                <a  href=\"http://www.fsdmfes.ac.ma/\">\n",
        "                    <img src=\"https://github.com/m-elkhou/Object-Detection/blob/master/assets/fsdm.png?raw=1\" width=75px />\n",
        "                </a>\n",
        "            </th>\n",
        "            <th>\n",
        "                <a href=\"https://www.univ-paris13.fr/\">\n",
        "                    <img src=\"https://github.com/m-elkhou/Object-Detection/blob/master/assets/uspn.png?raw=1\" width=150px/>\n",
        "                </a>\n",
        "            </th>\n",
        "            <th>\n",
        "                <a href=\"http://www.imperium-media.com/\">\n",
        "                    <img src=\"https://github.com/m-elkhou/Object-Detection/blob/master/assets/imperium_media.png?raw=1\" width=70px/>\n",
        "                </a>\n",
        "            </th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td style=\"text-align:center;\">\n",
        "                <a   href=\"http://www.fsdmfes.ac.ma/\">USMBA</a>\n",
        "            </td>\n",
        "            <td style=\"text-align:center;\">\n",
        "              <a   href=\"http://www.fsdmfes.ac.ma/\">FSDM</a>\n",
        "            </td>\n",
        "            <td style=\"text-align:center;\">\n",
        "                <a  href=\"https://www.univ-paris13.fr/\">USPN</a>\n",
        "            </td>\n",
        "            <td style=\"text-align:center;\">\n",
        "                <a  href=\"http://www.imperium-media.com/\">IMPERIUM MEDIA</a>\n",
        "            </td>\n",
        "        </tr>\n",
        "    </tbody>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EswX7FbB-t6H",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "#  **Object Detection with Deep Learning** #\n",
        "\n",
        "---\n",
        "\n",
        "<div align=\"left\" >\n",
        "<a href=\"https://colab.research.google.com/github/m-elkhou/Object-Detection/blob/master/Implementation.ipynb\">\n",
        "    <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\" title=\"Open and Execute in Google Colaboratory\" />\n",
        "</a>\n",
        "<a href=\"https://github.com/m-elkhou/Object-Detection/blob/master/Implementation.ipynb\">\n",
        "    <img align=\"left\" src=\"https://badgen.net/badge/icon/Open%20in%20GitHub?icon=github&label\" alt=\"Open in GitHub\" title=\"View source on GitHub\" />\n",
        "</a>\n",
        "<a href=\"https://drive.google.com/drive/folders/17BKVQkJXkOEa4omuy_PuBcXcrENhtLdC?usp=sharing\">\n",
        "<img src=\"https://raw.githubusercontent.com/m-elkhou/Object-Detection/master/assets/google-drive-badge.svg?sanitize=true\"/>\n",
        "<br/>\n",
        "</a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6kvL_WJ-t6I",
        "colab_type": "text"
      },
      "source": [
        "## Implementation Details:\n",
        "Here, we are going to explain the implementation steps for  different object detection models. To train the model on the custom dataset, standard implementation steps would be as below:\n",
        "1. Data Acquisition\n",
        "2. Data Labelling\n",
        "3. Data Preparation\n",
        "4. Model Training\n",
        "5. Evaluation / Results analysis\n",
        "\n",
        "### Data Acquisition\n",
        "\n",
        "Data acquisition is a required step when you want to train the model on the custom dataset. It will require more than 100 images with the object for which you want to train the model. Each image should have good quality, contains object/objects at least once (to be detected). No image should be repetitive within the dataset, the more the variety of images, the more extensive the training becomes.\n",
        "\n",
        "Single Shot Detectron (SSD) and Faster R-CNN require images with objects only, but in the case of YOLO, negative samples are also being used for the training and validation.\n",
        "\n",
        "Here, we are going to showcase the demo for the model training, and prediction of the custom object which is the logo of the brand VIVO. We have collected ~200 images for the training.\n",
        "\n",
        "### Data Labelling\n",
        "\n",
        "For the custom dataset, we need to label objects from each of the images. For object detection, the model requires certain details of the objects that are to be detected from the image and those are the X-axis, y-axis, height, and width of the object within the image.\n",
        "\n",
        "Run below command to install labelImg\n",
        "\n",
        "```python\n",
        "pip install labelImg\n",
        "```\n",
        "You can also refer to https://github.com/tzutalin/labelImg for the reference. The labelImg tool provides a user interface that allows a user to draw bounding boxes and captures the x-axis and y-axis along with the height and width of the bounding box. This also provides a feature to label multiple objects within a single image. labelImg stores axis values in a separate XML file at the same location where the image is present.\n",
        "\n",
        "**Data Preparation & Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Tcfl3fY_Db",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI6GnkLS74Cy",
        "colab_type": "text"
      },
      "source": [
        "### Open files from Google Drive\n",
        "\n",
        "mount your Google Drive in your virtual machine using an authorization code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zalqWu_R-7if",
        "colab_type": "code",
        "outputId": "091cdffb-7580-4f29-acd7-2ad35e1e82c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Colab Notebooks/Object_Detection\n",
        "\n",
        "home = '/content/drive/My Drive/Colab Notebooks/Object_Detection/' \n",
        "import sys\n",
        "sys.path.append(home)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/Object_Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU2z0ncApwEJ",
        "colab_type": "text"
      },
      "source": [
        "Installing the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A7dWdYK4tPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install -U -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmHzdlLEMgUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2, sys, os\n",
        "from glob import glob\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHU1CviBAvUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "tf = tarfile.open(home+'dataset/openlogo.tar')\n",
        "tf.extractall(path=home+'dataset/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5e5b2b2b-b2de-4219-e01d-bc66795f3990",
        "id": "ojd3PHNvm0E6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "imgs_path = glob(home+'dataset/openlogo/JPEGImages/'+\"*.jpg\")\n",
        "print(len(imgs_path))\n",
        "imgs_path = glob('dataset/openlogo/JPEGImages/'+\"*.jpg\")\n",
        "print(len(imgs_path))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27924\n",
            "27924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNV-0yLb_lSA",
        "colab_type": "text"
      },
      "source": [
        "# Faster R-CNN:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxOOW0VN-t6J",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation:\n",
        "\n",
        "1. Create a text file from the xml file which is generated from the labelImg where txt file where each image path and it’s relative bounding boxes co-ordinate in new line which is in the following format and move it to the cloned repository :\n",
        "```\n",
        "filepath,xmin,ymin,xmax,ymax,class_name\n",
        "```  \n",
        "\n",
        "## Training\n",
        "\n",
        "1. In order to implement the object detection using Faster R-CNN first we need to clone this GitHub repository:\n",
        "```\n",
        "git clone https://github.com/kbardool/keras-frcnn\n",
        "```\n",
        "2. Move the train_images and test_images folder to the cloned repository\n",
        "3. Now open the terminal in the repository folder and install requirements.txt file.\n",
        "4. We can train the model using Faster R-CNN through executing this line in the terminal:\n",
        "```\n",
        "python train_frcnn.py -o simple -p (path to txt file)\n",
        "```\n",
        "\n",
        "## Inference:\n",
        "1. Now put the testing images in the folder named test_images and in the same folder of Faster R-CNN and execute the following line in the terminal:\n",
        "python test_frcnn.py -p test_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68nvvYs8_p6t",
        "colab_type": "text"
      },
      "source": [
        "# Masck R-CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBtiDra2EGEc",
        "colab_type": "text"
      },
      "source": [
        "Download Mask-RCNN repository for GitHub - https://github.com/matterport/Mask_RCNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9skPJv0I_yr2",
        "colab_type": "code",
        "outputId": "d44dfaf0-040b-4423-8232-25a852fa354b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!git clone https://github.com/matterport/Mask_RCNN.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "remote: Enumerating objects: 956, done.\u001b[K\n",
            "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n",
            "Receiving objects: 100% (956/956), 111.82 MiB | 18.91 MiB/s, done.\n",
            "Resolving deltas: 100% (568/568), done.\n",
            "Checking out files: 100% (76/76), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqSvEhWwBUQM",
        "colab_type": "text"
      },
      "source": [
        "There is a script that automatically migrates old TF-1 code to TF-2 code. From terminal just execute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpM9ev4pBJ5A",
        "colab_type": "code",
        "outputId": "eabe632b-2b10-46e4-cefd-2fd7bba030d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!tf_upgrade_v2 --intree Mask_RCNN --inplace --reportfile report.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO line 287:62: Added keywords to args of function 'tf.shape'\n",
            "INFO line 324:55: Added keywords to args of function 'tf.shape'\n",
            "INFO line 325:24: Added keywords to args of function 'tf.pad'\n",
            "INFO line 341:11: Renamed 'tf.log' to 'tf.math.log'\n",
            "INFO line 341:23: Renamed 'tf.log' to 'tf.math.log'\n",
            "INFO line 399:17: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 431:44: Added keywords to args of function 'tf.shape'\n",
            "INFO line 439:43: Added keywords to args of function 'tf.shape'\n",
            "INFO line 445:27: Added keywords to args of function 'tf.shape'\n",
            "INFO line 445:48: Added keywords to args of function 'tf.shape'\n",
            "INFO line 466:35: Added keywords to args of function 'tf.shape'\n",
            "INFO line 467:26: Added keywords to args of function 'tf.shape'\n",
            "INFO line 482:32: Added keywords to args of function 'tf.shape'\n",
            "INFO line 482:53: Added keywords to args of function 'tf.shape'\n",
            "INFO line 509:29: Added keywords to args of function 'tf.shape'\n",
            "INFO line 518:19: Added keywords to args of function 'tf.boolean_mask'\n",
            "INFO line 520:35: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 526:15: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 527:19: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 538:20: Added keywords to args of function 'tf.reduce_max'\n",
            "INFO line 542:18: Added keywords to args of function 'tf.reduce_max'\n",
            "INFO line 545:23: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 547:23: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 553:23: Renamed 'tf.random_shuffle' to 'tf.random.shuffle'\n",
            "INFO line 554:21: Added keywords to args of function 'tf.shape'\n",
            "INFO line 558:23: Renamed 'tf.random_shuffle' to 'tf.random.shuffle'\n",
            "INFO line 565:28: Added keywords to args of function 'tf.cond'\n",
            "INFO line 566:19: Added keywords to args of function 'tf.shape'\n",
            "INFO line 567:26: Added keywords to args of function 'tf.argmax'\n",
            "INFO line 579:38: Added keywords to args of function 'tf.transpose'\n",
            "INFO line 597:26: Added keywords to args of function 'tf.shape'\n",
            "INFO line 611:8: Added keywords to args of function 'tf.shape'\n",
            "INFO line 612:49: Added keywords to args of function 'tf.shape'\n",
            "INFO line 613:11: Added keywords to args of function 'tf.pad'\n",
            "INFO line 614:19: Added keywords to args of function 'tf.pad'\n",
            "INFO line 615:23: Added keywords to args of function 'tf.pad'\n",
            "INFO line 616:13: Added keywords to args of function 'tf.pad'\n",
            "INFO line 617:12: Added keywords to args of function 'tf.pad'\n",
            "INFO line 700:16: Added keywords to args of function 'tf.argmax'\n",
            "INFO line 716:11: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 719:20: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 720:15: Renamed 'tf.sets.set_intersection' to 'tf.sets.intersection'\n",
            "INFO line 722:15: Renamed 'tf.sparse_tensor_to_dense' to 'tf.sparse.to_dense'\n",
            "INFO line 734:14: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 744:47: Added keywords to args of function 'tf.shape'\n",
            "INFO line 745:21: Added keywords to args of function 'tf.pad'\n",
            "INFO line 756:35: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 758:11: Renamed 'tf.sets.set_intersection' to 'tf.sets.intersection'\n",
            "INFO line 760:11: Renamed 'tf.sparse_tensor_to_dense' to 'tf.sparse.to_dense'\n",
            "INFO line 764:26: Added keywords to args of function 'tf.shape'\n",
            "INFO line 772:8: Changed tf.to_float call to tf.cast(..., dtype=tf.float32).\n",
            "INFO line 777:43: Added keywords to args of function 'tf.shape'\n",
            "INFO line 778:17: Added keywords to args of function 'tf.pad'\n",
            "INFO line 857:33: Added keywords to args of function 'tf.shape'\n",
            "INFO line 869:50: Added keywords to args of function 'tf.shape'\n",
            "INFO line 1035:14: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 1043:20: Added keywords to args of function 'tf.size'\n",
            "INFO line 1060:14: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 1072:20: Added keywords to args of function 'tf.size'\n",
            "INFO line 1093:21: Added keywords to args of function 'tf.argmax'\n",
            "INFO line 1108:11: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 1108:33: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 1126:22: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 1136:20: Added keywords to args of function 'tf.size'\n",
            "INFO line 1154:17: Added keywords to args of function 'tf.shape'\n",
            "INFO line 1156:17: Added keywords to args of function 'tf.shape'\n",
            "INFO line 1160:17: Added keywords to args of function 'tf.transpose'\n",
            "INFO line 1164:18: Renamed 'tf.where' to 'tf.compat.v1.where'\n",
            "INFO line 1175:20: Added keywords to args of function 'tf.size'\n",
            "INFO line 2173:16: Added keywords to args of function 'tf.reduce_mean'\n",
            "INFO line 2180:73: Added keywords to args of function 'tf.size'\n",
            "INFO line 2197:16: Added keywords to args of function 'tf.reduce_mean'\n",
            "INFO line 2822:24: Added keywords to args of function 'tf.reduce_sum'\n",
            "INFO line 2823:12: Added keywords to args of function 'tf.boolean_mask'\n",
            "INFO line 72:21: `name` passed to `name_scope`. Because you may be re-entering an existing scope, it is not safe to convert automatically,  the v2 name_scope does not support re-entering scopes by name.\n",
            "\n",
            "INFO line 72:21: Renamed 'tf.name_scope' to 'tf.compat.v1.name_scope'\n",
            "INFO line 132:8: Renamed 'tf.reset_default_graph' to 'tf.compat.v1.reset_default_graph'\n",
            "INFO line 202:9: Renamed 'tf.log' to 'tf.math.log'\n",
            "INFO line 203:9: Renamed 'tf.log' to 'tf.math.log'\n",
            "TensorFlow 2.0 Upgrade Script\n",
            "-----------------------------\n",
            "Converted 11 files\n",
            "Detected 0 issues that require attention\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Make sure to read the detailed log 'report.txt'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4X_-caj7CDOk",
        "colab_type": "text"
      },
      "source": [
        "- Go to: Mask_RCNN folder\n",
        "- Install the software "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBwu3iiDBfHv",
        "colab_type": "code",
        "outputId": "2b6022dd-b93b-439f-d79d-f6d15dc7ab09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "%cd Mask_RCNN\n",
        "!python setup.py install"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Object_Detection/Mask_RCNN\n",
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating mask_rcnn.egg-info\n",
            "writing mask_rcnn.egg-info/PKG-INFO\n",
            "writing dependency_links to mask_rcnn.egg-info/dependency_links.txt\n",
            "writing top-level names to mask_rcnn.egg-info/top_level.txt\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/mrcnn\n",
            "copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "copying mrcnn/config.py -> build/lib/mrcnn\n",
            "copying mrcnn/model.py -> build/lib/mrcnn\n",
            "copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-36.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/mask_rcnn-2.1-py3.6.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing mask_rcnn-2.1-py3.6.egg\n",
            "Copying mask_rcnn-2.1-py3.6.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding mask-rcnn 2.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Processing dependencies for mask-rcnn==2.1\n",
            "Finished processing dependencies for mask-rcnn==2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_79VZ4ZmBfU6",
        "colab_type": "code",
        "outputId": "b1ea031e-9490-4802-bd64-070ac427f3b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd .."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Object_Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syEkFYWGCIKE",
        "colab_type": "text"
      },
      "source": [
        "Confirm the Library Was Installed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnnGHC25Bfdv",
        "colab_type": "code",
        "outputId": "a7551098-98b6-49fa-f201-c3df702e05d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "!pip show mask-rcnn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: mask-rcnn\n",
            "Version: 2.1\n",
            "Summary: Mask R-CNN for object detection and instance segmentation\n",
            "Home-page: https://github.com/matterport/Mask_RCNN\n",
            "Author: Matterport\n",
            "Author-email: waleed.abdulla@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.6/dist-packages/mask_rcnn-2.1-py3.6.egg\n",
            "Requires: \n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTZQqi06Cfbw",
        "colab_type": "text"
      },
      "source": [
        "- Download Model Weights\n",
        "from : [mask_rcnn_coco.h5](https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5) (246 megabytes)\n",
        "- Move downloaded model to MASK_RCNN folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwXMr6JUCQy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# example of inference with a pre-trained coco model\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.model import MaskRCNN\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        " \n",
        "# draw an image with detected objects\n",
        "def draw_image_with_boxes(filename, boxes_list):\n",
        "    # load the image\n",
        "    data = plt.imread(filename)\n",
        "    # plot the image\n",
        "    plt.imshow(data)\n",
        "    # get the context for drawing boxes\n",
        "    ax = plt.gca()\n",
        "    # plot each box\n",
        "    for box in boxes_list:\n",
        "        # get coordinates\n",
        "        y1, x1, y2, x2 = box\n",
        "        # calculate width and height of the box\n",
        "        width, height = x2 - x1, y2 - y1\n",
        "        # create the shape\n",
        "        rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n",
        "        # draw the box\n",
        "        ax.add_patch(rect)\n",
        "    # show the plot\n",
        "    plt.show()\n",
        "\n",
        "# define the test configuration\n",
        "class TestConfig(Config):\n",
        "    NAME = \"test\"\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "    NUM_CLASSES = 1 + 80\n",
        "\n",
        "# define the model\n",
        "rcnn = MaskRCNN(mode='inference', model_dir='./', config=TestConfig())\n",
        "# load coco model weights\n",
        "rcnn.load_weights('mask_rcnn_coco.h5', by_name=True)\n",
        "# load photograph\n",
        "img = load_img('images/perfectwallpaper.jpg')\n",
        "img = img_to_array(img)\n",
        "# import cv2\n",
        "# img = cv2.imread('images/perfectwallpaper.jpg')\n",
        "# make prediction\n",
        "results = rcnn.detect([img], verbose=0)\n",
        "# visualize the results\n",
        "draw_image_with_boxes('images/perfectwallpaper.jpg', results[0]['rois'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jWjg2kJ_em8",
        "colab_type": "text"
      },
      "source": [
        "# SSD:\n",
        "\n",
        "We have referred the following link for the implementation of object detection using SSD:\n",
        "https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La87x3_L-t6K",
        "colab_type": "text"
      },
      "source": [
        "  \n",
        "## Data Preparation:\n",
        "\n",
        "1. Created .pbtxt file containing label names and it’s id.\n",
        "2. Then we converted all labelImg xml file to csv file using xml_to_csv.py.\n",
        "3. We then converted the CSV file to .record file format using generate_tfrecord.py.\n",
        "\n",
        "## Training:\n",
        "\n",
        "1. Then we had downloaded the pretrained model of ssd_inception and also created a config file according to tutorial with the following changes in the config file:\n",
        "```config\n",
        "num_classes: 1\n",
        "type: 'ssd_inception_v2' # Set to the name of your chosen pre- \\ trained\n",
        "model\n",
        "fine_tune_checkpoint: \"pre-trained-model/model.ckpt\" # Path to \\ extracted\n",
        "files of pre-trained model\n",
        "train_input_reader: { \\\n",
        "tf_record_input_reader { \\\n",
        "input_path: \"annotations/train.record\" # Path to training \\\n",
        "TFRecord file\n",
        "}\n",
        "label_map_path: \"annotations/label_map.pbtxt\" # Path to label \\ map\n",
        "file\n",
        "}\n",
        "```\n",
        "\n",
        "2. We can train the model through executing this line in the terminal:\n",
        "```\n",
        "python train.py --logtostderr --train_dir=training -- \\ pipeline_config_path=training/ssd_inception_v2_coco.config\n",
        "```\n",
        "\n",
        "## Inference:\n",
        "1. Copy the TensorFlow/models/research/object_detection/export_inference_graph.py script and paste it straight into your training_demo folder.\n",
        "2. Then using the highest checkpoint file of the model and generate .pb file by executing this line in the terminal:\n",
        "```\n",
        "python export_inference_graph.py --input_type image_tensor -- \\ pipeline_config_path training/ssd_inception_v2_coco.confi -- \\ trained_checkpoint_prefix training/model.ckpt-13302 - \\ output_directory trained-inference- \\ graphs/output_inference_graph_v1.pb\n",
        "```\n",
        "3. Using this object_detection code we can predict the test images for SSD:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nExNv2wi-t6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "# Path to frozen detection graph .pb file\n",
        "PATH_TO_CKPT = '.../frozen_inference_graph.pb'\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = '.../label_map.pbtxt' \n",
        "\n",
        "# Path to image\n",
        "PATH_TO_IMAGE = '.../test_images/394.jpg'\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = \\ label_map_util.convert_label_map_to_categories(label_map, \\ max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "    \n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Each box represents a part of the image where a particular \\ object was detected\n",
        "detection_boxes = \\ detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "detection_scores = \\ detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = \\ detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "num_detections = \\ detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "image = cv2.imread(PATH_TO_IMAGE)\n",
        "image_expanded = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Perform the actual detection by running the model with the image \\ as input\n",
        "(boxes, scores, classes, num) = sess.run(\n",
        "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "    feed_dict={image_tensor: image_expanded})\n",
        "    \n",
        "vis_util.visualize_boxes_and_labels_on_image_array( \\\n",
        "    image, \\\n",
        "    np.squeeze(boxes), \\\n",
        "    np.squeeze(classes).astype(np.int32), \\\n",
        "    np.squeeze(scores), \\\n",
        "    category_index, \\\n",
        "    use_normalized_coordinates=True, \\\n",
        "    line_thickness=8, \\\n",
        "    min_score_thresh=0.60)\n",
        "\n",
        "# All the results have been drawn on image. Now display the image.\n",
        "cv2.imshow('Object detector', image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GOQGsfO-t6P",
        "colab_type": "text"
      },
      "source": [
        "### Result Analysis\n",
        "\n",
        "In our use case, we have focused on the detection of the logo of the brand VIVO. We have annotated 150 images for the training. Each image has an average of 3 objects. We have captured training images from the different sports events in order to keep the variety of background within the train images. Also, we have captured 50 of the images, with quite a similar background. However, Faster R-CNN and SSD do not consider negative samples (samples without objects) for the training.\n",
        "\n",
        "For the comparative analysis as a first observation, we have trained the object detection model for 2K steps/ iteration on 150 images.\n",
        "\n",
        "Sample training images are as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKeoQpkC-t6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1-MXbL7Bazc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}